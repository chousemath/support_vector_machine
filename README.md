### What is a Classifier?

* You can think of a classifier (`분류자`) as a function (`함수`) that takes in some data as input, and returns a label as output
* Supervised learning: create a classifier by finding patterns in examples
* Classifiers are only as good as the features you provide it
* In machine learning, you generally need multiple features to work with, otherwise an if/else statement might be just as good as a classifier
* You need to avoid useless features (features that may appear useful by accident), otherwise it might actually hurt your classifier
* You also want your features to be fully independent (they should not be coupled in any way)
* Avoid redundant features
* You want your features to be easy to understand
* In Scikit-Learn, although there are many different types of classifiers, their python interfaces are almost identical
* One way to think of machine learning is to use training data to adjust the parameters of a machine learning model
* For images, it is extremely difficult to extract useful features by hand
* Deep learning is especially useful because you do not have to extract features manually, deep learning uses the raw pixels of each image, and the classifier takes care of the rest
* Good training data is high in diversity and quantity
* When we work with images, we use the raw pixels as features

### TensorFlow

* `Training accuracy`: shows the percentage of the images used in the current training batch that were labeled with the correct class
* `Validation accuracy`: the validation accuracy is the precision (percentage of correctly-labelled images) on a randomly-selected group of images from a different set
* `Cross entropy`: is a loss function that gives a glimpse into how well the learning process is progressing. (Lower numbers are better.)

### Sample Support Vector Machine

* A support vector machine is a classification technique
* A support vector machine is, at the end of the day, a model that best splits clusters of data
* SVMs are good at dealing with high-dimensional data, and works well on small datasets
* The distance between the data points and the svm model should be as wide as possible
* The points are called support vectors, and the `line` generated by the model is called the hyperplane
* Need to normalize the data

### What is the C Parameter?

* The C parameter tells the SVM optimization how much you want to avoid misclassifying each training example.
* For large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly.
* Conversely, a very small value of C will cause the optimizer to look for a larger-margin separating hyperplane, even if that hyperplane misclassifies more points.
* For very tiny values of C, you should get misclassified examples, often even if your training data is linearly separable.

### Terms

* Receptive field: what set of inputs makes the value of this neuron as high as it can possibly be
* Sigmoid squashing function: put a value in (x-value), where it crosses the sigmoid function is your output value (y-value), no matter where you are, the value is always between 1 and -1
* Rectified linear units: if value is negative, make it zero, if it is positive, keep the value

### Operations

* There is a Docker container with the GPU version of TensorFlow pre-installed

### Relevant Mathematical Concepts

* Constrained optimization
* Lagrange multipliers

![svm1](images/svm1.png)
![svm7](images/svm7.png)
![svm2](images/svm2.png)
![svm8](images/svm8.png)
![svm3](images/svm3.png)
![svm4](images/svm4.png)
![svm5](images/svm5.png)
![svm6](images/svm6.png)

### Resources

* https://www.youtube.com/watch?v=N1vOgolbjSc
* https://github.com/adashofdata/muffin-cupcake
* https://www.youtube.com/watch?v=ax8LxRZCORU

### Configuring MobileNet

* `resolution`: Feeding in a higher resolution image takes more processing time, but results in better classification accuracy
* `size`: Relative size of the model as a fraction of the largest MobileNet: 1.0, 0.75, 0.50, or 0.25

### Vocabulary

* 범주화 (Categorization)
* 분류 (Classification)
* 머신러닝 (Machine Learning)
* 협업 필터링 (Collaborative Filtering)

### Goal for D-Day

* 300 AI/ML/Math Tutorials (5/300)
* 30 Korean AI/ML News Articles (0/30)

### Training Commands

```bash
# start tensorboard
$ tensorboard --logdir tf_files/training_summaries &
# kill all existing tensorboard instances
$ pkill -f "tensorboard"
```

```bash
# retrain the model
# tensorflow demands all images by jpg
# You can very likely get improved results (i.e. higher accuracy) by training for longer (how_many_training_steps)
$ python retrain.py --bottleneck_dir=tf_files/bottlenecks --how_many_training_steps=4000 --model_dir=tf_files/models/ --summaries_dir=tf_files/training_summaries/"${ARCHITECTURE}" --output_graph=tf_files/retrained_graph.pb --output_labels=tf_files/retrained_labels.txt --architecture="${ARCHITECTURE}" --image_dir=tf_files/flower_photos
```

```bash
# actually use the model on my personal computer
$ python label_image.py --graph=/Users/jo/Desktop/data_science/support_vector_machine/machine_learning_recipes/tf_files/retrained_graph.pb --labels=/Users/jo/Desktop/data_science/support_vector_machine/machine_learning_recipes/tf_files/retrained_labels.txt --input_layer=Placeholder --output_layer=final_result --image=/Users/jo/Desktop/data_science/support_vector_machine/machine_learning_recipes/test_data/test.jpg
# performing the same command on a linux box
python label_image.py --graph=/home/contact/support_vector_machine/machine_learning_recipes/tf_files/retrained_graph.pb --labels=/home/contact/support_vector_machine/machine_learning_recipes/tf_files/retrained_labels.txt --input_layer=Placeholder --output_layer=final_result --image=/home/contact/support_vector_machine/machine_learning_recipes/test_data/test.jpg
```

### Using Anaconda on Google Compute Engine

```bash
# Using the `conda` binary
$ /home/contact/anaconda3/bin/conda INPUT_COMMAND_HERE
# Activating a virtual environment
$ source /home/contact/anaconda3/bin/activate ai
```

