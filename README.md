### What is a Classifier?

* You can think of a classifier (`분류자`) as a function (`함수`) that takes in some data as input, and returns a label as output
* Supervised learning: create a classifier by finding patterns in examples
* Classifiers are only as good as the features you provide it
* In machine learning, you generally need multiple features to work with, otherwise an if/else statement might be just as good as a classifier
* You need to avoid useless features (features that may appear useful by accident), otherwise it might actually hurt your classifier
* You also want your features to be fully independent (they should not be coupled in any way)
* Avoid redundant features
* You want your features to be easy to understand
* In Scikit-Learn, although there are many different types of classifiers, their python interfaces are almost identical
* One way to think of machine learning is to use training data to adjust the parameters of a machine learning model

### Sample Support Vector Machine

* A support vector machine is a classification technique
* A support vector machine is, at the end of the day, a model that best splits clusters of data
* SVMs are good at dealing with high-dimensional data, and works well on small datasets
* The distance between the data points and the svm model should be as wide as possible
* The points are called support vectors, and the `line` generated by the model is called the hyperplane
* Need to normalize the data

### What is the C Parameter?

* The C parameter tells the SVM optimization how much you want to avoid misclassifying each training example.
* For large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly.
* Conversely, a very small value of C will cause the optimizer to look for a larger-margin separating hyperplane, even if that hyperplane misclassifies more points.
* For very tiny values of C, you should get misclassified examples, often even if your training data is linearly separable.

### Terms

* Receptive field: what set of inputs makes the value of this neuron as high as it can possibly be
* Sigmoid squashing function: put a value in (x-value), where it crosses the sigmoid function is your output value (y-value), no matter where you are, the value is always between 1 and -1
* Rectified linear units: if value is negative, make it zero, if it is positive, keep the value

### Relevant Mathematical Concepts

* Constrained optimization
* Lagrange multipliers

![svm1](images/svm1.png)
![svm7](images/svm7.png)
![svm2](images/svm2.png)
![svm8](images/svm8.png)
![svm3](images/svm3.png)
![svm4](images/svm4.png)
![svm5](images/svm5.png)
![svm6](images/svm6.png)

### Resources

* https://www.youtube.com/watch?v=N1vOgolbjSc
* https://github.com/adashofdata/muffin-cupcake
* https://www.youtube.com/watch?v=ax8LxRZCORU

### Vocabulary

* 범주화 (Categorization)
* 분류 (Classification)
* 머신러닝 (Machine Learning)
* 협업 필터링 (Collaborative Filtering)

### Goal for D-Day

* 300 AI/ML/Math Tutorials (4/300)
* 30 Korean AI/ML News Articles (0/30)

